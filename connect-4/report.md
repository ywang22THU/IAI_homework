# 四子棋 实验报告

## 实现方式

在本次实验中，我采用了蒙特卡洛搜索来实现，具体代码位于`MCT.h`与`MCT.cpp`中，分析如下：

### 节点类与蒙特卡洛树类

定义了`Node`类代表蒙特卡洛树节点，`MCT`类代表蒙特卡洛树，具体定义如下（其中省略了构造函数与析构函数）

```C++
class Node{               // 蒙特卡洛树的节点
    Node * parent;        // 父节点
    Node ** childs;       // 子节点
    int M, N;             // 棋盘行数与列数 
    int newX, newY;       // 相比于父节点新增的点
    int owner;            // 该格的拥有者
    int winner;           // 该格对应棋局的胜者
    double UCB;           // 节点信心上限
    double sumProfit;     // 总收益，用于计算UCB
    double times;         // 访问次数，用于计算UCB
    int expables;         // 尚未被拓展的数量
    int expnum;           // 初始状态下能被拓展的数量
    bool * expd;          // 已经被拓展的列
    int * top;            // 每列最顶层
};

class MCT{                // 蒙特卡洛树
    Node * root;          // 树的根节点
    int M, N;             // 棋盘大小
    int noX, noY;         // 禁止点
    int ** initBoard;     // 初始棋盘
    int ** board;         // 当前棋局
public:
    double UCBcacl(Node *);               // 计算一个节点的UCB
    Node * choose();                      // 选择节点
    Node * bestChild(Node *);             // 选择UCB最大的孩子
    Node * winChild();                    // 选择根节点的子节点中胜率最大的一个（决定走步）
    Node * expand(Node *);                // 扩展节点
    pair<double, int> simulate(Node *);   // 模拟节点
    void back(Node *, pair<double, int>); // 回传
    Point search();                       // 蒙特卡洛搜索
};
```

每一棵蒙特卡洛树用于决定一个棋局的走步，因此在每次`getPoint`函数被调用的时候新建一棵树并在返回前删除之。每一个节点代表了初始棋局一种可能的趋势，理论上其代表的也是一整盘棋，但是这样整棵树所占的空间过大，因此采用增量法，即每个节点只记录其相对于父节点新增的节点。  
这样我们只需要在树中保持一份初始化的棋局，一盘推演棋局（不断变化）即可，有效的减少了程序的空间占用。

### 搜索方法

采用传统的选择-扩展-模拟-回传的思路，具体来说：  
首先选择到一个尚未扩展完毕的节点，扩展之并对扩展节点进行模拟，将模拟的结果回传

#### 选择

采用多臂老虎机模型，每次需要选择子节点中UCB值最大的一个，直到到达一个可以拓展的节点，UCB值的计算公式为：
$$
\text{UCB}(v) = \frac{\text{profit}(v)}{n(v)} + C\sqrt{\frac{2\ln(N)}{n(v)}}
$$
其中，参数含义如下：
- $\text{profit}(v)$：当前节点的收益
- $n(v)$：当前节点的访问次数
- $N$：整棵树的访问次数
- $C$：调节参数

#### 扩展

当遇到可扩展的节点时，停止选择并进行扩展，扩展的方式为随机在一个可以下的点走棋，之后更新推演棋盘，并对新棋盘进行模拟  
当随机选择的点能够直接决定胜负或平局时，则更新其该点的胜者、收益与相关信息，胜平负分别收益为$1, -1, -1$，由于平局在验收中的得分比胜利要低，因此直接将平局视为失败处理

#### 模拟

模拟棋局，即双方随机落子直到决出胜负，这里给出了额外的判断条件，即无论当前落子方是我方还是敌方，都允许他们进行三子攻防策略，如下
1. 如果落子方可以落子后直接胜利，则下在此处
2. 如果落子方在不落在某一处会导致下一回合对方可以直接胜利，则下在此处
 
如果不满足这两个条件，则随机落子。
在模拟的过程中，我让双方都采用了上述策略，而不仅让我方采用，这是为了让对手更强，让蒙特卡洛在模拟的过程中能够更好的模拟对手足够强的情况，如此便能更好的训练我的AI。

#### 回传

回传的过程需要将模拟的收益回传给被模拟的节点所有的直接祖先，考虑到如果模拟的次数过多，该棋局在现实中出现的概率可能很小，因此采用了按比例衰减的策略，即每$15$次模拟会将收益衰减至原来的$0.9$倍，胜平负均会相应的衰减

#### 最终决策

最终，我选取了根节点的子节点中，胜率最大的节点作为走步，在尝试了胜率最大、UCB最大等策略过后，我发现胜率最大是最为合理的
因为UCB取决于胜率与探索度两项，探索度只是为了帮助树更快的扩张，促进更多局面杯模拟，因此在决定走步的过程中选择胜率而非UCB

## 本地测试结果

本地测试一共进行了$5$轮，每轮验收与$50$个AI比赛$100$局，平均取得了$94\%$的胜率，例如最后一轮的测试结果如下：
|id|wina|winb|losea|loseb|tie|bug|debug|illegal|deillegal|timeout|detimeout|
|--|----|----|-----|-----|---|---|-----|-------|---------|-------|---------|
2022010760|47|44|2|3|0|0|2|0|0|0|2|

在saiblo上测试的过程中，也取得了$95\%$的胜率，如下图，在与$100$号AI等强力AI的对战过程中经常取得全胜的战绩，而面对一些棋艺比较糟糕的AI更是有摧枯拉朽横扫千军之势。

![远程批量测试结果](./saiblob.png)

## 一些尝试

在大体策略确定之后，我也进行了一些额外了的尝试，但是对于胜率都没有质的提升，因此在最终没有采用

### 中间取点

在一局棋盘中，显然中间的点相对于周围的点更具有扩展的可能性，在我们现实中下棋的时候也更倾向于在中间下棋，因此我修改了模拟过程中随机选子的概率，使其不是完全均匀而是相对更倾向于选择中间的列，具体来说，我尝试了以下两种权重，分别是
$$
\begin{align*}
1, 2, \dots k, k, \dots, 2, 1\qquad\text{n is even}\\
1, 2, \dots k, k+1, k, \dots, 2, 1\qquad\text{n is odd}
\end{align*}
$$
和
$$
\begin{align*}
1, 3, 5, \dots 2k - 1, 2k - 1, \dots, 5, 3, 1\qquad\text{n is even}\\
1, 3, 5\dots 2k - 1, 2k + 1, 2k - 1, \dots, 5, 3, 1\qquad\text{n is odd}
\end{align*}
$$
但是在实际测试中，发现这两种随机方式相对于均匀随机来说的胜率都不是很好，我猜想其原因可能是模拟的足够充分，消除了中间取点的影响

### 调参

我的实现过程中，参数一共有三组，一是UCB值计算过程中的$C$值，一是胜平负的收益权重，另一组是衰减比例

#### C值

在其余条件均不变的情况下，我将$C$值在$0.5-1.0$中以$0.1$的步长进行了测试，结果发现在这个范围内其对于胜率没有明显影响，仅有$2\%$左右的浮动，因此最终选择了中间值$0.7$  
这个没有太大影响的原因我认为是，$C$值决定了蒙特卡洛会优先探索还是优先重复高胜节点，而在搜索次数足够多的情况下，这个比例反而不会有太大影响

#### 胜平负收益权重

我尝试了以下几组权重，每个进行了一轮本地测试，结果如下：
|参数|结果|
|---|---|
|(2, 1, 0)|87%|
|(3, 1, 0)|83%|
|(2, 1, -1)|89%|
|(1, -1, -1)|95%|
|(2, -1, -1)|91%|

我猜想其中存在一些原因：
- 当将平与负取成相同的权重时结果明显更好，因为对于得分统计来说，胜才是最优解
- 由于最终选取了衰减的策略，因此如果出现了$0$这个权重，其在衰减的过程中不会变化，会导致最终选择出现问题

#### 衰减比例

我尝试在$0.6-0.9$之间调整了衰减比例，衰减比例是决定了我们要抛弃多少高度模拟棋局的结果，最终发现不应过分衰减，否则在棋局初期无法实现很好的布局，导致后期无法行棋

## 总结

通过这次实验，我独立实现了蒙特卡洛树与其搜索过程，对其有了深刻的理解，包括底层逻辑、生效原因等，同时也引入了一些自己的逻辑，帮助AI更快的学习，了解到了在强化学习的过程中人类监督的作用

### 建议

建议`Judge.cpp`中提供的判断胜利函数更换一下函数名称，因为在我写完之后跟同学们的交流过程中，发现有很多同学对于`user`和`machine`有误解，导致其逻辑出现重大缺陷，我认为可以采用`SelfAI`和`Adversary`之类的命名